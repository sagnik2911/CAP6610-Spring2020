{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment II-4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L7Vq6eDSYxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHM8Dip6TFlM",
        "colab_type": "text"
      },
      "source": [
        "We test the performance of three regression methods on the wine data set http://archive.ics.uci.edu/ml/datasets/Wine+Quality. We will only consider the red wine data set, with 1599 samples. We use the first 1400 samples for training, and the last 199 samples for testing. The goal is to build a linear model of the first 11 features (together with a constant term) to predict the quality of the wine. All models are trained by solving the following optimization problem.  $$\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "\\underset{w,\\beta}{\\text{minimize}}\n",
        "\\sum_{i=1}^{n} l(x_i^{T}w+\\beta - y_i)\n",
        "\\end{aligned}\n",
        "\\end{equation*}$$ \n",
        "where the loss functions are\n",
        "\n",
        "• least squares loss $ l(t) = t^{2} $\n",
        "\n",
        "• Huber loss defined in the previous problem, with M = 1\n",
        "\n",
        "• deadzone-linear loss $$ l(t) = \\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t0  & \\mbox{if } |t| \\leq 0.5 \\\\\n",
        "\t\t|t|- 0.5 & \\mbox{if } |t| > 0.5\n",
        "\t\\end{array}\n",
        "\\right. $$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQuLGutTYVuB",
        "colab_type": "code",
        "outputId": "88e3eb24-ea92-4693-899c-2d5ac2bbe0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# importing libraries and reading data from file\n",
        "import pandas as pnd\n",
        "import numpy as nmp\n",
        "dataFile = pnd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep = ';')\n",
        "dataFile.shape\n",
        "dataFile.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.4              0.70         0.00  ...       0.56      9.4        5\n",
              "1            7.8              0.88         0.00  ...       0.68      9.8        5\n",
              "2            7.8              0.76         0.04  ...       0.65      9.8        5\n",
              "3           11.2              0.28         0.56  ...       0.58      9.8        6\n",
              "4            7.4              0.70         0.00  ...       0.56      9.4        5\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOCwG0LVYhOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "featureColNames  = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']\n",
        "outputName = ['quality']\n",
        "X_features = dataFile[featureColNames].values\n",
        "y = dataFile[outputName].values\n",
        "#The data sample of 1599 records being split into train set(1400) and test set(199)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.124, random_state=42, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDk30iVL3My5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying Linear regression to fit least square and using the same to predict test data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X_train,y_train) \n",
        "y_prediction = reg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtVGNbb25vvO",
        "colab_type": "code",
        "outputId": "6d7d2506-21f3-4f13-e14d-cea8f3168341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Calculating error rate\n",
        "error = 0\n",
        "for i in range (0,199):\n",
        "  error += np.abs(y_prediction[i] - y_test[i]) \n",
        "\n",
        "error /= 199\n",
        "print('Mean Absolution Error: '+ str(error))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolution Error: [0.53296711]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baGF46XELkSl",
        "colab_type": "code",
        "outputId": "9ea0093a-2591-4691-ff6b-4bf67b6f2063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "# Applying huber loss in Convex Optimization form for building the model and using the same to predict test data\n",
        "import cvxpy as cp\n",
        "# Weight vector is of size 11, as we are selecting all 11 features of our data\n",
        "weight = cp.Variable((11,1))\n",
        "b = cp.Variable(1)\n",
        "#The value of M is 1\n",
        "cost = cp.sum(cp.huber((X_train@weight + b) - y_train, 1))\n",
        "prob = cp.Problem(cp.Minimize(cost))\n",
        "prob.solve(verbose = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------\n",
            "           OSQP v0.6.0  -  Operator Splitting QP Solver\n",
            "              (c) Bartolomeo Stellato,  Goran Banjac\n",
            "        University of Oxford  -  Stanford University 2019\n",
            "-----------------------------------------------------------------\n",
            "problem:  variables n = 4212, constraints m = 4200\n",
            "          nnz(P) + nnz(A) = 26482\n",
            "settings: linear system solver = qdldl,\n",
            "          eps_abs = 1.0e-05, eps_rel = 1.0e-05,\n",
            "          eps_prim_inf = 1.0e-04, eps_dual_inf = 1.0e-04,\n",
            "          rho = 1.00e-01 (adaptive),\n",
            "          sigma = 1.00e-06, alpha = 1.60, max_iter = 10000\n",
            "          check_termination: on (interval 25),\n",
            "          scaling: on, scaled_termination: off\n",
            "          warm start: on, polish: on, time_limit: off\n",
            "\n",
            "iter   objective    pri res    dua res    rho        time\n",
            "   1  -2.2400e+04   8.00e+00   5.24e+06   1.00e-01   7.69e-03s\n",
            " 100   5.3542e+02   4.44e-07   1.38e-07   1.51e-01   3.94e-02s\n",
            "plsh   5.3542e+02   2.05e-14   2.28e-11   --------   4.62e-02s\n",
            "\n",
            "status:               solved\n",
            "solution polish:      successful\n",
            "number of iterations: 100\n",
            "optimal objective:    535.4178\n",
            "run time:             4.62e-02s\n",
            "optimal rho estimate: 2.05e-01\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "535.4177751956599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74s7xwwNEU9",
        "colab_type": "code",
        "outputId": "27d8c1e7-c7fc-4fc9-e8f6-dbbda746f2af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Applying the weight vector generated by train data to test model\n",
        "y_prediction = np.dot(X_test , weight.value) + b.value\n",
        "error = 0\n",
        "for i in range (0,199):\n",
        "  error += np.abs(y_prediction[i] - y_test[i]) \n",
        "error /= 199\n",
        "\n",
        "print('Mean Absolution Error: '+ str(error))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolution Error: [0.53271565]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqq2Z4iXODEw",
        "colab_type": "code",
        "outputId": "e7a456d1-b936-4f8e-9320-1ece9b5e954c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Applying Dead-zone linear loss in Convex Optimization form for building the model and using the same to predict test data \n",
        "\n",
        "weight = cp.Variable((11,1))\n",
        "b = cp.Variable(1)\n",
        "cost = cp.maximum(0,(cp.abs((X_train@weight + b)- y_train) - 0.5))\n",
        "obj = cp.sum(cost)\n",
        "prob = cp.Problem(cp.Minimize(obj))\n",
        "prob.solve(solver=cp.ECOS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210.32700397403707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYvGgiaoPS9A",
        "colab_type": "code",
        "outputId": "ae57dcb8-e95f-49e2-fbc9-32104f219f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# testing the model\n",
        "y_prediction = nmp.dot(X_test , weight.value) + b.value\n",
        "error = 0\n",
        "for i in range (0,199):\n",
        "  error += np.abs(y_prediction[i] - y_test[i]) \n",
        "error /= 199\n",
        "\n",
        "print('Mean Absolution Error: '+ str(error))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolution Error: [0.54810579]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}